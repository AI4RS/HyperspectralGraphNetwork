{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c24fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation dgcnn.pytorch\n",
    "# Experiment 12: Testing\n",
    "# 07.11.2022\n",
    "# Read txt for description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a850a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 3 streams then concatenate after first block\n",
    "# OA = %\n",
    "\n",
    "# Define model\n",
    "# from model.py\n",
    "# COPY THIS BLOCK FOR TESTING !!\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    " \n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_graph_feature_rgb(x, k=20, idx=None, dim9=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    # print('x shape', x.size())\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            # idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "            # Use normXYZ + RGB + SWIR + geo for knn search\n",
    "            idx = knn(x[:,3:], k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    \n",
    "    # only use RGB for graph feature\n",
    "    x = x[:,6:9,:] # RGB\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n",
    "\n",
    "def get_graph_feature_swir(x, k=20, idx=None, dim9=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    # print('x shape', x.size())\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            # idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "            # Use normXYZ + RGB + SWIR + geo for knn search\n",
    "            idx = knn(x[:,3:], k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    \n",
    "    # only use SWIR for graph feature\n",
    "    x = x[:,9:153,:] # SWIR\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n",
    "\n",
    "def get_graph_feature_geo(x, k=20, idx=None, dim9=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    # print('x shape', x.size())\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            # idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "            # Use normXYZ + RGB + SWIR + geo for knn search\n",
    "            idx = knn(x[:,3:], k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    \n",
    "    # only use geo for graph feature\n",
    "    x = x[:,153:181,:] # geo\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n",
    "\n",
    "def get_graph_feature(x, k=20, idx=None, dim9=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n",
    "\n",
    "class DGCNN_semseg(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DGCNN_semseg, self).__init__()\n",
    "        self.args = args\n",
    "        self.k = args_k\n",
    "        \n",
    "        self.bn1_rgb = nn.BatchNorm2d(64)\n",
    "        self.bn2_rgb = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn1_swir = nn.BatchNorm3d(4)\n",
    "        self.bn2_swir = nn.BatchNorm3d(4)\n",
    "        self.bn3_swir = nn.BatchNorm2d(64)\n",
    "        self.bn4_swir = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.bn1_geo = nn.BatchNorm2d(64)\n",
    "        self.bn2_geo = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(64)        \n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "        self.bn7 = nn.BatchNorm1d(512)\n",
    "        self.bn8 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        \n",
    "        # RGB\n",
    "        self.conv1_rgb = nn.Sequential(nn.Conv2d(dim_rgb*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1_rgb,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2_rgb = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2_rgb,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        # SWIR\n",
    "        self.conv1_swir = nn.Sequential(nn.Conv3d(1, 4, kernel_size=(32,1,1), bias=False),\n",
    "                                   self.bn1_swir,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))      \n",
    "        self.conv2_swir = nn.Sequential(nn.Conv3d(4, 4, kernel_size=(32,1,1), bias=False),\n",
    "                                   self.bn2_swir,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3_swir = nn.Sequential(nn.Conv2d(dim_swir*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn3_swir,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))      \n",
    "        self.conv4_swir = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn4_swir,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        \n",
    "        # geo\n",
    "        self.conv1_geo = nn.Sequential(nn.Conv2d(dim_geo*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1_geo,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2_geo = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2_geo,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        \n",
    "        # all\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(196*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv6 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn6,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        self.conv7 = nn.Sequential(nn.Conv1d(192, 512, kernel_size=1, bias=False),\n",
    "                                   self.bn7,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv8 = nn.Sequential(nn.Conv1d(512, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn8,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.dp1 = nn.Dropout(p=args_dropout)\n",
    "        self.conv9 = nn.Conv1d(256, args_num_class, kernel_size=1, bias=False) # CHANGE TO NUMBER OF CLASSES\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_points = x.size(2)\n",
    "        \n",
    "        # RGB\n",
    "        x_rgb = get_graph_feature_rgb(x, k=self.k, dim9=False)   # (batch_size, 9, num_points) -> (batch_size, 9*2, num_points, k) \n",
    "        x_rgb = self.conv1_rgb(x_rgb)                       # (batch_size, 9*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x_rgb = self.conv2_rgb(x_rgb)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x_rgb = x_rgb.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "        \n",
    "        # SWIR\n",
    "        x_swir3d = get_graph_feature_swir(x, k=self.k, dim9=False)   # (batch_size, 9, num_points) -> (batch_size, 9*2, num_points, k)\n",
    "        x_swir3d = x_swir3d.unsqueeze(1)\n",
    "        x_swir3d = self.conv1_swir(x_swir3d)                       # (batch_size, 9*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x_swir3d = self.conv2_swir(x_swir3d)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x_swir3d = x_swir3d.max(dim=2,keepdim=False)[0]\n",
    "        x_swir3d = x_swir3d.max(dim=-1,keepdim=False)[0]\n",
    "        \n",
    "        x_swir = get_graph_feature_swir(x, k=self.k, dim9=False)   # (batch_size, 9, num_points) -> (batch_size, 9*2, num_points, k)\n",
    "        x_swir = self.conv3_swir(x_swir)\n",
    "        x_swir = self.conv4_swir(x_swir)\n",
    "        x_swir = x_swir.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "        \n",
    "        # geo\n",
    "        x_geo = get_graph_feature_geo(x, k=self.k, dim9=False)   # (batch_size, 9, num_points) -> (batch_size, 9*2, num_points, k)\n",
    "        x_geo = self.conv1_geo(x_geo)                       # (batch_size, 9*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x_geo = self.conv2_geo(x_geo)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x_geo = x_geo.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "        \n",
    "        x_all_cat = torch.cat((x_rgb, x_swir3d, x_swir, x_geo), dim=1) # RGB + SWIR + geo # 192 + swir3d features\n",
    "        \n",
    "        x_all_graph_0 = get_graph_feature(x_all_cat, k=self.k)\n",
    "        x_all_1 = self.conv1(x_all_graph_0)\n",
    "        x_all_2 = self.conv2(x_all_1)\n",
    "        x_all_2_max = x_all_2.max(dim=-1, keepdim=False)[0]\n",
    "        \n",
    "        x_all_graph_1 = get_graph_feature(x_all_2_max, k=self.k)\n",
    "        x_all_3 = self.conv3(x_all_graph_1)\n",
    "        x_all_4 = self.conv4(x_all_3)\n",
    "        x_all_4_max = x_all_4.max(dim=-1, keepdim=False)[0]\n",
    "        \n",
    "        x_all_graph_2 = get_graph_feature(x_all_4_max, k=self.k)\n",
    "        x_all_5 = self.conv5(x_all_graph_2)\n",
    "        x_all_6 = self.conv6(x_all_5)\n",
    "        x_all_6_max = x_all_6.max(dim=-1, keepdim=False)[0]\n",
    "        \n",
    "        x_cat = torch.cat((x_all_2_max, x_all_4_max, x_all_6_max), dim=1)\n",
    "        \n",
    "        x_fc1 = self.conv7(x_cat)                       # (batch_size, 64*3, num_points) -> (batch_size, 512, num_points)\n",
    "        x_fc2 = self.conv8(x_fc1)                       # (batch_size, 512, num_points) -> (batch_size, 256, num_points)\n",
    "        x_dp = self.dp1(x_fc2)\n",
    "        x_end = self.conv9(x_dp)                       # (batch_size, 256, num_points) -> (batch_size, num_class, num_points)\n",
    "        \n",
    "        return x_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c8ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def room2blocks(data, label, num_point, block_size=1.0, stride=1.0,\n",
    "                random_sample=False, sample_num=None, sample_aug=1, use_all_points=False):\n",
    "    \"\"\" Prepare block training data.\n",
    "    Args:\n",
    "        data: N x 6 numpy array, 012 are XYZ in meters, 345 are RGB in [0,1]\n",
    "            assumes the data is shifted (min point is origin) and aligned\n",
    "            (aligned with XYZ axis)\n",
    "        label: N size uint8 numpy array from 0-12\n",
    "        num_point: int, how many points to sample in each block\n",
    "        block_size: float, physical size of the block in meters\n",
    "        stride: float, stride for block sweeping\n",
    "        random_sample: bool, if True, we will randomly sample blocks in the room\n",
    "        sample_num: int, if random sample, how many blocks to sample\n",
    "            [default: room area]\n",
    "        sample_aug: if random sample, how much aug\n",
    "    Returns:\n",
    "        block_datas: K x num_point x 6 np array of XYZRGB, RGB is in [0,1]\n",
    "        block_labels: K x num_point x 1 np array of uint8 labels\n",
    "        \n",
    "    TODO: for this version, blocking is in fixed, non-overlapping pattern.\n",
    "    \"\"\"\n",
    "    assert (stride <= block_size)\n",
    "\n",
    "    limit = np.amax(data, 0)[0:3]\n",
    "\n",
    "    # Get the corner location for our sampling blocks    \n",
    "    xbeg_list = []\n",
    "    ybeg_list = []\n",
    "    if not random_sample:\n",
    "        num_block_x = int(np.ceil((limit[0] - block_size) / stride)) + 1\n",
    "        num_block_y = int(np.ceil((limit[1] - block_size) / stride)) + 1\n",
    "        for i in range(num_block_x):\n",
    "            for j in range(num_block_y):\n",
    "                xbeg_list.append(i * stride)\n",
    "                ybeg_list.append(j * stride)\n",
    "    else:\n",
    "        num_block_x = int(np.ceil(limit[0] / block_size))\n",
    "        num_block_y = int(np.ceil(limit[1] / block_size))\n",
    "        if sample_num is None:\n",
    "            sample_num = num_block_x * num_block_y * sample_aug\n",
    "        for _ in range(sample_num):\n",
    "            xbeg = np.random.uniform(-block_size, limit[0])\n",
    "            ybeg = np.random.uniform(-block_size, limit[1])\n",
    "            xbeg_list.append(xbeg)\n",
    "            ybeg_list.append(ybeg)\n",
    "\n",
    "    # Collect blocks\n",
    "    block_data_list = []\n",
    "    block_label_list = []\n",
    "    idx = 0\n",
    "    for idx in range(len(xbeg_list)):\n",
    "        xbeg = xbeg_list[idx]\n",
    "        ybeg = ybeg_list[idx]\n",
    "        xcond = (data[:, 0] <= xbeg + block_size) & (data[:, 0] >= xbeg)\n",
    "        ycond = (data[:, 1] <= ybeg + block_size) & (data[:, 1] >= ybeg)\n",
    "        cond = xcond & ycond\n",
    "        if np.sum(cond) < 20:  # discard block if there are less than 20 pts. # Check !!\n",
    "            if np.sum(cond) > 0:\n",
    "                print('Discard block!! Number of points = ', np.sum(cond))\n",
    "            continue\n",
    "\n",
    "        block_data = data[cond, :]\n",
    "        block_label = label[cond]\n",
    "\n",
    "        if use_all_points:\n",
    "            block_data_list.append(block_data)\n",
    "            block_label_list.append(block_label)\n",
    "        else:\n",
    "            # randomly subsample data\n",
    "            block_data_sampled, block_label_sampled = \\\n",
    "                sample_data_label(block_data, block_label, num_point)\n",
    "            block_data_list.append(np.expand_dims(block_data_sampled, 0))\n",
    "            block_label_list.append(np.expand_dims(block_label_sampled, 0))\n",
    "\n",
    "    if use_all_points:\n",
    "        block_data_return, block_label_return = np.array(block_data_list), np.array(block_label_list)\n",
    "    else:\n",
    "        block_data_return, block_label_return = np.concatenate(block_data_list, 0), np.concatenate(block_label_list, 0)\n",
    "    print('block_data_return_size:')\n",
    "    print(np.array(block_data_return).shape)\n",
    "\n",
    "    return block_data_return, block_label_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41680f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class S3DISDataset_eval(Dataset):  # load data block by block, without using h5 files\n",
    "    def __init__(self, split='train', data_root='trainval_fullarea', num_point=4096, test_area='5', block_size=1.0, stride=1.0, num_class=20, use_all_points=False, num_thre = 1024):\n",
    "        super().__init__()\n",
    "        self.num_point = num_point\n",
    "        self.block_size = block_size\n",
    "        self.use_all_points = use_all_points\n",
    "        self.stride = stride\n",
    "        self.num_thre = num_thre\n",
    "        rooms = sorted(os.listdir(data_root))\n",
    "        rooms = [room for room in rooms if 'Area_' in room]\n",
    "        if split == 'train':\n",
    "            rooms_split = [room for room in rooms if not 'Area_{}'.format(test_area) in room]\n",
    "        else:\n",
    "            rooms_split = [room for room in rooms if 'Area_{}'.format(test_area) in room]\n",
    "        self.room_points, self.room_labels = [], []\n",
    "        self.room_coord_min, self.room_coord_max = [], []\n",
    "\n",
    "        room_idxs = []\n",
    "        for index, room_name in enumerate(rooms_split):\n",
    "            room_path = os.path.join(data_root, room_name)\n",
    "            room_data = np.load(room_path)\n",
    "            points, labels = room_data[:, 0:-1], room_data[:, -1] # CHECK !! label always in the last column !!\n",
    "            coord_min, coord_max = np.amin(points, axis=0)[:3], np.amax(points, axis=0)[:3]\n",
    "            self.room_coord_min.append(coord_min), self.room_coord_max.append(coord_max)\n",
    "            block_points, block_labels = room2blocks(points, labels, self.num_point, block_size=self.block_size,\n",
    "                                                       stride=self.stride, random_sample=False, sample_num=None, use_all_points=self.use_all_points)\n",
    "            room_idxs.extend([index] * int(block_points.shape[0]))  # extend with number of blocks in a room\n",
    "            self.room_points.append(block_points), self.room_labels.append(block_labels)\n",
    "        self.room_points = np.concatenate(self.room_points)\n",
    "        self.room_labels = np.concatenate(self.room_labels)\n",
    "\n",
    "        self.room_idxs = np.array(room_idxs)\n",
    "        print(\"Totally {} samples in {} set.\".format(len(self.room_idxs), split))\n",
    "\n",
    "    def __getitem__(self, idx):  # get items in one block\n",
    "        room_idx = self.room_idxs[idx]\n",
    "        selected_points = self.room_points[idx]   # num_point * XYZ RGB SWIR geo\n",
    "        current_labels = self.room_labels[idx]   # num_point\n",
    "        center = np.mean(selected_points, axis=0)\n",
    "        N_points = selected_points.shape[0]\n",
    "\n",
    "        current_points = np.zeros((N_points, data_dimension+6))  # data dimension + XYZ + normXYZ, Check!\n",
    "        # add normalized XYZ (column 3,4,5)\n",
    "        current_points[:, 3] = selected_points[:, 0] / self.room_coord_max[room_idx][0]\n",
    "        current_points[:, 4] = selected_points[:, 1] / self.room_coord_max[room_idx][1]\n",
    "        current_points[:, 5] = selected_points[:, 2] / self.room_coord_max[room_idx][2]\n",
    "        # recenter for each block\n",
    "        selected_points[:, 0] = selected_points[:, 0] - center[0]\n",
    "        selected_points[:, 1] = selected_points[:, 1] - center[1]\n",
    "        # normalize RGB\n",
    "        selected_points[:, 3:6] /= 255.0\n",
    "        # SWIR already normalized\n",
    "        # Colummn: XYZ, normXYZ, RGB, SWIR, geo\n",
    "        current_points[:,0:3] = selected_points[:,0:3] # for XYZ\n",
    "        current_points[:,6:9] = selected_points[:,3:6] # for RGB\n",
    "        current_points[:,9:153] = selected_points[:,6:150] # for SWIR\n",
    "        current_points[:,153:181] = selected_points[:,150:178] # for geo\n",
    "\n",
    "        return current_points, current_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.room_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9098a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sem_IoU(pred_np, seg_np, num_classes):  # num_classes: S3DIS 13\n",
    "    I_all = np.zeros(num_classes)\n",
    "    U_all = np.zeros(num_classes)\n",
    "    for sem_idx in range(len(seg_np)):\n",
    "        for sem in range(num_classes):\n",
    "            I = np.sum(np.logical_and(pred_np[sem_idx] == sem, seg_np[sem_idx] == sem))\n",
    "            U = np.sum(np.logical_or(pred_np[sem_idx] == sem, seg_np[sem_idx] == sem))\n",
    "            I_all[sem] += I\n",
    "            U_all[sem] += U\n",
    "    return I_all / U_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecfd64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "# from data import S3DISDataset\n",
    "# from data import S3DISDataset_eval\n",
    "# from model import DGCNN_semseg\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "# from util import cal_loss, IOStream\n",
    "import sklearn.metrics as metrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test():\n",
    "    DUMP_DIR = path_dump_dir\n",
    "    all_true_cls = []\n",
    "    all_pred_cls = []\n",
    "    all_true_seg = []\n",
    "    all_pred_seg = []\n",
    "\n",
    "    dataset = S3DISDataset_eval(split='test', data_root=args_data_dir, num_point=args_num_points, test_area=args_test_area,\n",
    "                           block_size=args_block_size, stride=args_block_size, num_class=args_num_classes, num_thre=100, use_all_points=True)\n",
    "    test_loader = DataLoader(dataset, batch_size=args_test_batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    room_idx = np.array(dataset.room_idxs)\n",
    "    num_blocks = len(room_idx)\n",
    "\n",
    "    fout_data_label = []\n",
    "    for room_id in np.unique(room_idx):\n",
    "        out_data_label_filename = 'Area_%s_pred_gt_%s.txt' % (test_area, args_predict_name)\n",
    "        out_data_label_filename = os.path.join(DUMP_DIR, out_data_label_filename)\n",
    "        fout_data_label.append(open(out_data_label_filename, 'w+'))     \n",
    "\n",
    "    device = torch.device(\"cuda\" if args_cuda else \"cpu\")\n",
    "\n",
    "    # io.cprint('Start overall evaluation...')\n",
    "\n",
    "    # Try to load models\n",
    "    if args_model == 'dgcnn':\n",
    "        model = DGCNN_semseg(nn.Module).to(device)\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "    model = model.eval()\n",
    "\n",
    "    print('model restored')\n",
    "\n",
    "    test_acc = 0.0\n",
    "    count = 0.0\n",
    "    test_true_cls = []\n",
    "    test_pred_cls = []\n",
    "    test_true_seg = []\n",
    "    test_pred_seg = []\n",
    "\n",
    "    print('Start testing ...')\n",
    "    num_batch = 0\n",
    "    for data, seg in tqdm(test_loader):\n",
    "        th_subblock = 20000\n",
    "        st_sb = 0\n",
    "        en_sb = th_subblock\n",
    "        if data.shape[1] > th_subblock:\n",
    "            print('Too many points in the block. Split the block!!')\n",
    "            \n",
    "            # Split data into n sub-blocks\n",
    "            n_subblocks = int(np.ceil(data.shape[1]/th_subblock))\n",
    "            print('N subblocks', n_subblocks)\n",
    "            for split in range(n_subblocks):\n",
    "                print('Working on subblock = ', split+1)\n",
    "                if split+1 < n_subblocks:\n",
    "                    n_pts_subblock = th_subblock\n",
    "                else:\n",
    "                    n_pts_subblock = data.shape[1] - (split*th_subblock) \n",
    "                \n",
    "                data_split = torch.zeros([1,n_pts_subblock,data.shape[2]],dtype=torch.float64)\n",
    "                seg_split = torch.zeros([1,n_pts_subblock],dtype=torch.float64)\n",
    "                data_split[:,:,:] = data[:,st_sb:en_sb,:]\n",
    "                seg_split[:,:] = seg[:,st_sb:en_sb]\n",
    "                data_split, seg = data_split.to(device), seg.to(device)\n",
    "                data_split = data_split.permute(0, 2, 1).float()\n",
    "                batch_size = data_split.size()[0]\n",
    "                \n",
    "                st_sb += th_subblock\n",
    "                en_sb += th_subblock\n",
    "\n",
    "                seg_pred = model(data_split)\n",
    "                seg_pred = seg_pred.permute(0, 2, 1).contiguous()\n",
    "                pred = seg_pred.max(dim=2)[1]\n",
    "                seg_np = seg_split.cpu().numpy()\n",
    "                pred_np = pred.detach().cpu().numpy()\n",
    "                test_true_cls.append(seg_np.reshape(-1))\n",
    "                test_pred_cls.append(pred_np.reshape(-1))\n",
    "                test_true_seg.append(seg_np)\n",
    "                test_pred_seg.append(pred_np)\n",
    "\n",
    "                # write prediction results\n",
    "\n",
    "                for batch_id in range(batch_size):\n",
    "                    pts = data_split[batch_id, :, :]\n",
    "                    pts = pts.permute(1, 0).float()\n",
    "                    l = seg_split[batch_id, :]\n",
    "                    pts[:, 6:9] *= 255.0 # unnormalized RGB, previously in 3:6\n",
    "                    pred_ = pred[batch_id, :]\n",
    "                    logits = seg_pred[batch_id, :, :]\n",
    "                    # compute room_id\n",
    "                    room_id = room_idx[num_batch + batch_id]\n",
    "                    for i in range(pts.shape[0]):\n",
    "                        fout_data_label[room_id].write('%f %f %f %d %d %d %d %d\\n' % (\n",
    "                           # change the position of normXYZ from 6,7,8 to 3,4,5\n",
    "                           pts[i, 3]*dataset.room_coord_max[room_id][0], pts[i, 4]*dataset.room_coord_max[room_id][1], pts[i, 5]*dataset.room_coord_max[room_id][2],\n",
    "                           pts[i, 6], pts[i, 7], pts[i, 8], pred_[i], l[i]))  # xyzRGB pred gt\n",
    "                \n",
    "\n",
    "        else:\n",
    "            data, seg = data.to(device), seg.to(device)\n",
    "            data = data.permute(0, 2, 1).float()\n",
    "            batch_size = data.size()[0]\n",
    "\n",
    "            seg_pred = model(data)\n",
    "            seg_pred = seg_pred.permute(0, 2, 1).contiguous()\n",
    "            pred = seg_pred.max(dim=2)[1]\n",
    "            seg_np = seg.cpu().numpy()\n",
    "            pred_np = pred.detach().cpu().numpy()\n",
    "            test_true_cls.append(seg_np.reshape(-1))\n",
    "            test_pred_cls.append(pred_np.reshape(-1))\n",
    "            test_true_seg.append(seg_np)\n",
    "            test_pred_seg.append(pred_np)\n",
    "\n",
    "            # write prediction results\n",
    "\n",
    "            for batch_id in range(batch_size):\n",
    "                pts = data[batch_id, :, :]\n",
    "                pts = pts.permute(1, 0).float()\n",
    "                l = seg[batch_id, :]\n",
    "                pts[:, 6:9] *= 255.0 # unnormalized RGB, previously in 3:6\n",
    "                pred_ = pred[batch_id, :]\n",
    "                logits = seg_pred[batch_id, :, :]\n",
    "                # compute room_id\n",
    "                room_id = room_idx[num_batch + batch_id]\n",
    "                for i in range(pts.shape[0]):\n",
    "                    fout_data_label[room_id].write('%f %f %f %d %d %d %d %d\\n' % (\n",
    "                       # change the position of normXYZ from 6,7,8 to 3,4,5\n",
    "                       pts[i, 3]*dataset.room_coord_max[room_id][0], pts[i, 4]*dataset.room_coord_max[room_id][1], pts[i, 5]*dataset.room_coord_max[room_id][2],\n",
    "                       pts[i, 6], pts[i, 7], pts[i, 8], pred_[i], l[i]))  # xyzRGB pred gt\n",
    "            \n",
    "        num_batch += batch_size\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    for room_id in np.unique(room_idx):\n",
    "        fout_data_label[room_id].close()\n",
    "\n",
    "    # test_ious = calculate_sem_IoU(test_pred_cls, test_true_cls, args_num_classes)\n",
    "    test_true_cls = np.concatenate(test_true_cls)\n",
    "    test_pred_cls = np.concatenate(test_pred_cls)\n",
    "    # test_acc = metrics.accuracy_score(test_true_cls, test_pred_cls)\n",
    "    # avg_per_class_acc = metrics.balanced_accuracy_score(test_true_cls, test_pred_cls)\n",
    "    # test_pred_seg = np.concatenate(test_pred_seg, axis=0)\n",
    "    # outstr = 'Test :: test area: %s, test acc: %.6f, test avg acc: %.6f, test iou: %.6f' % (test_area,\n",
    "    #                                                                                        test_acc,\n",
    "    #                                                                                        avg_per_class_acc,\n",
    "    #                                                                                        np.mean(test_ious))\n",
    "    # io.cprint(outstr)\n",
    "\n",
    "    # calculate confusion matrix\n",
    "    conf_mat = metrics.confusion_matrix(test_true_cls, test_pred_cls)\n",
    "    print('Confusion matrix:')\n",
    "    print(conf_mat)\n",
    "    np.savetxt('predict_3DCNN/con_mat.txt', conf_mat)\n",
    "    \n",
    "    # calculate overall accuracy\n",
    "    OA = metrics.accuracy_score(test_true_cls, test_pred_cls)\n",
    "    print('Overall Accuracy')\n",
    "    print(OA)\n",
    "    # np.savetxt('predict/OA.txt', OA)\n",
    "    # io.cprint(str(conf_mat))\n",
    "\n",
    "    # all_true_cls.append(test_true_cls)\n",
    "    # all_pred_cls.append(test_pred_cls)\n",
    "    # all_true_seg.append(test_true_seg)\n",
    "    # all_pred_seg.append(test_pred_seg)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01316f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discard block!! Number of points =  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22077/2317239236.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  block_data_return, block_label_return = np.array(block_data_list), np.array(block_label_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_data_return_size:\n",
      "(198,)\n",
      "Totally 198 samples in test set.\n",
      "model restored\n",
      "Start testing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/198 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead\n",
      "  warnings.warn(\"is_namedtuple is deprecated, please use the python checks instead\")\n",
      " 28%|██▊       | 55/198 [01:53<04:50,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 56/198 [01:57<06:09,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 61/198 [02:15<07:41,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 75/198 [02:54<03:57,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 77/198 [03:02<05:51,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 82/198 [03:13<04:17,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 88/198 [03:30<04:18,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 100/198 [03:58<02:35,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 110/198 [04:24<02:44,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 119/198 [04:43<02:10,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 122/198 [04:51<02:55,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 123/198 [04:55<03:24,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 126/198 [05:00<02:12,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 131/198 [05:18<03:29,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 132/198 [05:23<04:01,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 136/198 [05:34<02:38,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 137/198 [05:40<03:32,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 138/198 [05:45<03:48,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 141/198 [05:54<03:01,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 142/198 [05:58<03:10,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on subblock =  2\n",
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 143/198 [06:04<03:42,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 150/198 [06:18<01:04,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 151/198 [06:24<02:06,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 152/198 [06:28<02:28,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 157/198 [06:47<02:24,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 158/198 [06:52<02:37,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 159/198 [06:57<02:52,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 161/198 [07:05<02:37,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 162/198 [07:09<02:30,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 166/198 [07:19<01:35,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 167/198 [07:24<01:48,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 169/198 [07:32<01:47,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 174/198 [07:44<01:01,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 175/198 [07:48<01:10,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n",
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 185/198 [08:17<00:35,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points in the block. Split the block!!\n",
      "N subblocks 2\n",
      "Working on subblock =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 186/198 [08:21<00:36,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on subblock =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [08:34<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[194579   3092     92   9067   2331  56913    311     37    512   1062]\n",
      " [  2877  17053      8    773     80     86      0      3    925    629]\n",
      " [  8966    356  90092   7571    780   6949     26   1103   1459   4735]\n",
      " [ 35751  25415  65596 583715   3183  41610   4375   3253   2276   3489]\n",
      " [ 10377    106   1163   7543   8216    875     24     43     45    221]\n",
      " [ 11990    230   1364   9155      7 599965  39457   3413    491   1780]\n",
      " [   240    342    214   3858      4  87023 196130   1345    142    566]\n",
      " [  1933   2699    690  15603    565   8170   1388 100844   9380   9026]\n",
      " [  2900   1962    482   9007    582    706     36   6562  45174  16128]\n",
      " [   640   3944   3481   9294    333    344     64  12073   9568 103896]]\n",
      "Overall Accuracy\n",
      "0.7621670197211479\n"
     ]
    }
   ],
   "source": [
    "args_data_dir = 'data/lithonet_sem_seg_data_Experiment_12' # CHANGE\n",
    "args_num_points = 4096\n",
    "args_test_area = '2'\n",
    "test_area = 2\n",
    "args_block_size = 50 # CHANGE # CUDA out of memory for 100 m block size\n",
    "args_num_classes = 10 # CHANGE\n",
    "args_test_batch_size = 1\n",
    "args_cuda = True\n",
    "args_model = 'dgcnn'\n",
    "args_k = 20\n",
    "args_emb_dims = 1024\n",
    "args_dropout = 0.5\n",
    "args_num_class = 10\n",
    "args_predict_name = 'Experiment_12'\n",
    "\n",
    "data_dimension = 175 # RGB (3) SWIR (144) geo (28)\n",
    "dim_rgb = 3\n",
    "dim_swir = 144\n",
    "dim_geo = 28\n",
    "\n",
    "path_dump_dir = 'predict_3DCNN'\n",
    "path_model = 'model_3DCNN/Experiment_12_best_acc.t7'\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d53ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
